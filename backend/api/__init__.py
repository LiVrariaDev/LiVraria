"""
Backend API module initialization
LangChainãƒ™ãƒ¼ã‚¹ã®LLMçµ±åˆ
"""
import os
import logging

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logger = logging.getLogger("uvicorn.error")

# LangChainãƒ™ãƒ¼ã‚¹ã®LLMé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# LangChainãƒ™ãƒ¼ã‚¹ã®LLMé–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# server.pyãªã©ã§ç›´æ¥llm.pyã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ã€ã“ã“ã§ã¯ã‚¨ã‚¤ãƒªã‚¢ã‚¹å®šç¾©ã®ã¿å‰Šé™¤
from .llm import llm_summary as summary_function

# LLMãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®é¸æŠï¼ˆGeminiã¾ãŸã¯Ollamaï¼‰
LLM_BACKEND = os.getenv("LLM_BACKEND", "gemini")

if LLM_BACKEND == "ollama":
	logger.info(f"ğŸ”§ [LLM Backend] LangChain + Ollama (model: {os.getenv('OLLAMA_MODEL', 'llama3.2')})")
else:
	logger.info("ğŸ”§ [LLM Backend] LangChain + Gemini")

__all__ = ['summary_function', 'LLM_BACKEND']
